{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0206bf92bd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebbrowser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import webbrowser\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "#from selenium import webdriver\n",
    "#from splinter import Browser\n",
    "\n",
    "!{sys.executable} -m pip install splinter\n",
    "\n",
    "\n",
    "\n",
    "#!conda install --yes --prefix {sys.prefix} splinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA MARs News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the connection, grabbing the page\n",
    "\n",
    "web_data = uReq(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as file is too large it can crash the console. so close the file.\n",
    "page_html = web_data.read()\n",
    "web_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML parsing step\n",
    "\n",
    "page = soup(page_html, \"html.parser\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the paragraph description form the inspect page\n",
    "\n",
    "description = page.find_all(\"div\",class_=\"rollover_description_inner\")\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of paragraph and clean the data to remove unwanted characters.\n",
    "desc_list=[]\n",
    "for i in range(len(description)):\n",
    "    x= description[i].text\n",
    "    desc_list.append(x.strip(\"\\n\"))\n",
    "    \n",
    "    \n",
    "\n",
    "print(desc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into the dtaframe.\n",
    "\n",
    "news_p = pd.DataFrame({'para_text':desc_list})\n",
    "\n",
    "news_p\n",
    "\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the title of all the articles\n",
    "title = page.find_all(\"div\", class_=\"content_title\")\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching articles as list and cleaning the data.\n",
    "news_title= []\n",
    "\n",
    "for i in range(len(title)):\n",
    "    y=title[i].text\n",
    "    news_title.append(y.strip(\"\\n\\n\"))\n",
    "\n",
    "print(news_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPL Mars Space Images - Features Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visit the url for JPL Featured Space Image here.\n",
    "url_jpl = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "web_jpl_data = uReq(url_jpl)\n",
    "\n",
    "page_html = web_jpl_data.read()\n",
    "web_jpl_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML parsing step\n",
    "\n",
    "jpl_page = soup(page_html, \"html.parser\")\n",
    "jpl_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PArsing to get the image url\n",
    "image = jpl_page.find_all(\"div\",class_=\"carousel_items\")\n",
    "print(image)\n",
    "\n",
    "#final_space_image_url = \"https://www.jpl.nasa.gov/\"+image\n",
    "\n",
    "#final_space_image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find image and strip the extra characters in url to fetch the background image\n",
    "\n",
    "image = jpl_page.find(\"article\")['style'].strip('background-image: url').strip(\"('/'\")\n",
    "#print(image)\n",
    "\n",
    "image_url = image.strip(\"'); \")\n",
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpl_base_url = \"https://www.jpl.nasa.gov/\"\n",
    "\n",
    "\n",
    "final_image_url = jpl_base_url + image_url\n",
    "print(final_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images= image.find('a')\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the url\n",
    "tweet_url= \"https://twitter.com/marswxreport?lang=en\"\n",
    "weather_data = uReq(tweet_url)\n",
    "\n",
    "#Read the data\n",
    "weather_page_html = weather_data.read()\n",
    "weather_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse the HTML data \n",
    "weather_page = soup(weather_page_html, \"html.parser\")\n",
    "#print(weather_page.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for tweet text\n",
    "\n",
    "mars_weather_data = weather_page.find_all('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")\n",
    "\n",
    "mars_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a for loop to get text fron mars_weather_data\n",
    "\n",
    "mars_weather=[]\n",
    "\n",
    "for i in  range(len(mars_weather_data)):\n",
    "    #print(mars_weather[0].text)\n",
    "    z = mars_weather_data[i].text\n",
    "    mars_weather.append(z)\n",
    "\n",
    "type(mars_weather)\n",
    "print(str(mars_weather))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather_text=\"Sol \"\n",
    "\n",
    "for i in mars_weather_data:\n",
    "    #print(mars_weather[0].text)\n",
    "    #z = mars_weather[i].text\n",
    "    if mars_weather_text in i.text:\n",
    "        mars_weather_text=i.text\n",
    "    #mars_weather_text.append(z)\n",
    "    \n",
    "print(mars_weather_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit Mars facts page and fetch the url\n",
    "\n",
    "space_url = \"https://space-facts.com/mars/\"\n",
    "space_req = uReq(space_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read the data\n",
    "space_data = space_req.read()\n",
    "space_req.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML data \n",
    "space_page = soup(space_data, \"html.parser\")\n",
    "print(space_page.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping table data directly from url\n",
    "\n",
    "tables = pd.read_html(space_url)\n",
    "table = tables[0]\n",
    "type(table)\n",
    "table.columns=[\"Parameter\",\"values\"]\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the column of the dataframe\n",
    "df=table.rename(columns={\"0\": \"Parameter\",\n",
    "                \"1\":\"Values\"})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the data into html string\n",
    "\n",
    "df.to_html('Marsweatherfacts.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARS Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Astrogeology url\n",
    "\n",
    "astro_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    \n",
    "    \n",
    "    \n",
    "#astro_url = \"https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\"\n",
    "astro_req = uReq(astro_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read the data\n",
    "astro_data = astro_req.read()\n",
    "astro_req.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML data \n",
    "astro_page = soup(astro_data, \"html.parser\")\n",
    "print(astro_page.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open ea h page of the hemisphere\n",
    "\n",
    "webbrowser.open(astro_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_hemisphere_url= astro_page.find(\"a\", class_=\"itemLink product-item\")[\"href\"]\n",
    "\n",
    "#image1 =image_data.find(\"a\")[\"href\"]\n",
    "image_hemisphere_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "astro_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "  \n",
    "    \n",
    "browser.visit(astro_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=True)\n",
    "base_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(astro_url)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "# Run loop to fetch full hemisphere image.\n",
    "\n",
    "# Store the base_ul \n",
    "hemispheres_base_url = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "browser.visit(astro_url)\n",
    "\n",
    "html_hemisphere_data = browser.html\n",
    "\n",
    "\n",
    "soup1 = bs(html, 'html.parser')\n",
    "\n",
    "# Retreive all items that contain mars hemispheres information\n",
    "items = soup1.find_all('div', class_='item')\n",
    "\n",
    "# Create empty list for hemisphere urls \n",
    "hemisphere_urls = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop through the items previously stored\n",
    "for i in items: \n",
    "    # Store title\n",
    "    title = i.find('h3').text\n",
    "    \n",
    "    # Store link that leads to full image website\n",
    "    partial_img_url = i.find('a', class_='itemLink product-item')['href']\n",
    "    \n",
    "    # Visit the link that contains the full image website \n",
    "    browser.visit(hemispheres_base_url + partial_img_url)\n",
    "    \n",
    "    # HTML Object of individual hemisphere information website \n",
    "    partial_img_html = browser.html\n",
    "    \n",
    "    # Parse HTML with Beautiful Soup for every individual hemisphere information website \n",
    "    soup2 = bs( partial_img_html, 'html.parser')\n",
    "    \n",
    "    #print(soup2.find_all('img', class_='wide-image')[0])\n",
    "    \n",
    "    # Retrieve full image source \n",
    "    img_url = hemispheres_base_url + soup2.find_all('img', class_='wide-image')[0]\n",
    "    \n",
    "    # Append the retreived information into a list of dictionaries \n",
    "    hemisphere_urls.append({\"title\" : title, \"img_url\" : img_url})\n",
    "    \n",
    "\n",
    "# Display hemisphere_urls\n",
    "#hemisphere_urls\n",
    "#print(partial_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=True)\n",
    "base_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(astro_url)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hem_img_urls = []\n",
    "hem_dict = {'title': [], 'img_url': [],}\n",
    "\n",
    "x = soup.find_all('h3')\n",
    "#print(x)\n",
    "\n",
    "for i in x:\n",
    "    t = i.get_text()\n",
    "    title = t.strip('Enhanced')\n",
    "    browser.click_link_by_partial_text(t)\n",
    "    url = browser.find_link_by_partial_href('download')['href']\n",
    "    hem_dict = {'title': title, 'img_url': url}\n",
    "    hem_img_urls.append(hem_dict)\n",
    "    browser.visit(base_url)\n",
    "print(hem_img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hem_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
